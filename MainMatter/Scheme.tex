\chapter{Esquema de la recuperaci\'on de im\'agenes}\label{chapter:SCHEME} %PRA = Patches reordering algorithm

\section{Esquema b\'asico}
Planteemos primeramente el esquema principal de esta propuesta para la recuperaci\'on de im\'agenes. Para ello consideremos la siguiente notaci\'on:

\begin{itemize}
	\item $\Z$: matriz que representa la imagen incompleta a recuperar, con dimensiones $N_1 \times N_2$, $N_1N_2 = N$.
	\item $\z$: versión en forma de vector(o señal) de la matriz $\Z$, con dimensiones $N \times 1$.
	\item $\P$: matriz de permutaci\'on de dimensiones $N \times N$.
\end{itemize}

Para obtener la una señal recuperada $\yhat$ a partir de $\z$ se procede de la siguiente forma:

\begin{equation}
	\begin{array}[t]{ccccccccc}
	& & \mbox{\tiny{Permutaci\'on}} & & \mbox{\tiny{Operador de suavidad}} & & \mbox{\tiny{Permutaci\'on inversa}} & &\\
	\z & \longrightarrow & \boxed{\P} & \longrightarrow & \boxed{\H} & \longrightarrow & \boxed{\P^{-1}} & \longrightarrow & \yhat \\
	\end{array}
	\label{basic:scheme}
\end{equation}

Se reordenan los elementos de $\z$ seg\'un $\P$, y a la señal resultante $\z^p$ se le aplica un operador de suavidad $\H$ con el cual se obtienen los elementos faltantes, los elementos de dicho resultado se permutan nuevamente a su pocioci\'on inicial mendiate $\P^{-1}$. Dicho vector final ser\'ia $\yhat$. Lo anterior puede expresarse como:

\begin{equation}
\yhat = \P^{-1}\H(\z^p) = \P^{-1}\H(\P\z)
\label{basic:formula}
\end{equation}

Para comprender mejor lo que se quiere lograr con este procedimiento supongamos que contamos con la imagen real, la versi\'on de $\Z$ con la informaci\'on todos sus p\'ixeles. Aunque en la pr\'actica esta imagen no existe, la utilizaremos para reflejar de forma clara el funcionamiento del esquema principal (\ref{basic:scheme}).

\begin{itemize}
	\item $\Y$: matriz de la imagen original, es igual a $\Z$ solo que si tiene el valor de sus p\'ixeles faltantes. 
	\item $\y$: versión en forma de vector(o señal) de la matriz $\Y$.
\end{itemize}

Supongamos que la matriz de permutaci\'on $\P$ tiene la propiedad de que al ser aplicada a $\y$ se obtiene una señal suave $\y^p$. Entonces, dado que $\z^p$ y $\y^p$ solo difieren en los p\'ixeles faltantes de $\Z$, y que $\H(z^p)$ completa la señal, haciéndola suave; se puede esperar que $\yhat$ sea una aproximaci\'on de $\y$. Dicho formalmente:

\begin{equation}
	\def\arraystretch{2}
	\begin{array}{llrl}
	&                                        &     \H (\P\z ) &\approx \P\y        \\ 
	&\Longrightarrow                         & P^{-1}\H(\P\z) &\approx \P^{-1}\P\y \\
	&\overset{(\ref{basic:formula})}{\Longrightarrow} & \yhat &\approx \y          \\
	\end{array}
	\label{P:explain}
\end{equation}

Llegados a este punto las únicas incógnitas son qu\'e operador $\H$ usar, y c\'omo obtener la matriz $\P$. Operadores para suavizar en \textbf{1D} hay varios en la literatura, ya sea los que se usan en interpolaci\'on o filtrado; por el momento dejemos este operador como un superpar\'ametro del algoritmo. Se explicar\'a entonces c\'omo hacer para obtener $\P$.

\section{La matriz de permutaci\'on}

El resultado (\ref{P:explain}) tiene como condici\'on que $\y^p$ sea una señal suave. Para calcular la suavidad de $\y^p$ según (referencia a la formula de suavidad) ser\'ia:

\begin{equation}
\|\y^p\|_{\mathit{TV}} = \sum_{j = 2}^{N}|\y_j^{p} - \y_{j - 1}^{p}|
\label{y:smoothness}
\end{equation}

La matriz $\P$ que buscamos es tal que minimiza $\|\y^p\|_{\mathit{TV}}$, recordemos que no se cuenta con la señal  $\y$, entonces calcular $|\y_j^{p} - \y_{j - 1}^{p}|$ solo es posible cuando ambos elementos est\'an en la imagen incompleta $\Z$, lo que significa que conocemos su valor. Para encontrar la distancia entre dos p\'ixeles de $\Z$ (arbitrariamente de si se conoce su valor o no) usaremos sus parches de la siguiente manera:

\begin{equation}
|\y_j^{p} - \y_{j - 1}^{p}| \equiv \omega(\x_i^p,\; \x_{i - 1}^p)
\label{patches:equiv}
\end{equation}

Donde $\x_i^p$ denota el parche de $\Z$ cuyo p\'ixel central es denotado por $\z_i^p$. Y $\omega$ es una funci\'on de distancia definida sobre los parches la cual cumple que para cualesquiera dos parches, proximidad entre ellos sugiere proximidad entre sus p\'ixeles centrales, justo como se expresa en (\ref{patches:equiv}). Luego, el problema de minimizar $\|\y^p\|_{\mathit{TV}}$ por (\ref{y:smoothness}) y (\ref{patches:equiv}) es equivalente a minimizar:

\begin{equation}
\|\X^p\|_{\mathit{TV}} = \sum_{j = 2}^{N}\omega(\x_j^{p},\; \x_{j - 1}^{p})
\end{equation}

Donde $\X$ denota el vector de los parches de $\Z$ y $\X^p$ una permutaci\'on de $\X$ dada por una matriz $\P$. En conclusi\'on: la matriz $\P$ que buscamos es aquella  que minimiza $\|\X^p\|_{\mathit{TV}}$. Ahora bien, lo anterior se puede replantear de la siguiente forma. Si consideramos el grafo ponderado cuyos nodos son todos los parches de $\Z$ y la arista que une cada par de nodos $\x_i,\;\x_j$ tiene peso $\omega(\x_i,\; \x_j)$; debemos resolver una instancia del problema \textbf{NP}-completo del viajero, conocido como \TSP (del ingl\'es \textit{traveling salesman problem}) buscando un camino que comience en un parche cualquiera, pase por el resto de los parches una \'unica vez y cuya longitud es m\'inima.

\section{Algoritmo de reordenamiento de parches}


\begin{equation}
	\z\;\left\{
	\def\arraystretch{2.2}
	\begin{array}{ccccccccc}
		\longrightarrow & \boxed{\P_1} & \longrightarrow & \boxed{\H} & \longrightarrow & \boxed{\P_1^{-1}} & \longrightarrow \\
		\longrightarrow & \boxed{\P_2} & \longrightarrow & \boxed{\H} & \longrightarrow & \boxed{\P_2^{-1}} & \longrightarrow \\
		\longrightarrow & \huge\vdots &  & \huge\vdots &  & \huge\vdots & \longrightarrow & \\
		\longrightarrow & \boxed{\P_K} & \longrightarrow & \boxed{\H} & \longrightarrow & \boxed{\P_K^{-1}} & \longrightarrow
	\end{array}
	\right\}\;\oplus\longrightarrow\boxed{\times\frac{1}{K}}\longrightarrow\yhat
\end{equation}

%	Ejemplo de algoritmo:
%	
%	\begin{algorithm}
%	    \caption{Algoritmo de Arnoldi para construir una base ortonormal del subespacio de Krylov $\mathcal{K}_\mf(\tau A,b)$}
%	    \label{alg:Arnoldi}
%	    \KwIn{Matriz $A\in \mathbb{R}^{d\times d}$, vector $b\in \mathbb{R}^{d}$ y constante $\tau$}
%	    \KwOut{Base ortonormal $\{v_1,v_2,\ldots,v_\mf,v_{\mf+1}\}$ de $K_\mf(\tau A,b)$ y matriz de Hessenberg $H_\mf=V_\mf^{\intercal}\tau A V_\mf $,
%	        donde $V_{\mf+1}=[v_1\,v_2\,\cdots \,v_\mf\,v_{\mf+1}]\in \mathbb{R}^{d\times \mf+1}$, $breakdown$ }
%	    $breakdown=false$\\
%	    $v_1=b/\lVert b \rVert_2$\\
%	    \For{ $j=1,2,\ldots,\mf$ }{
%	        $w_j=\tau A v_j$\\
%	        \For{ $i=1,\ldots,j$}{ 
%	            $\hf_{ij}=\langle w_j,v_i \rangle$\\
%	            $w_j=w_j-\hf_{ij}v_i$       
%	        }
%	        $\hf_{j+1,i}=\lVert w_j \rVert_2$\\
%	        \eIf(\tcp*[h] \emph{Break Down}\label{alg:breakdown}){$\hf_{j+1,i}<2\epsilon_{mach}$}{
%	            $\mf_{cut}=j$\\
%	             $breakdown=true$\\
%	            Stop
%	        }{
%	            $v_{j+1}=w_j/\hf_{j+1,i}$
%	        }
%	    }
%	\end{algorithm}


