\chapter{Esquema de la recuperaci\'on de im\'agenes}\label{chapter:SCHEME}

\section{Esquema inicial}
Planteemos primeramente el esquema principal de esta propuesta para la recuperaci\'on de im\'agenes. Para ello consideremos la siguiente notaci\'on:

\begin{itemize}
	\item $\Z$: matriz que representa la imagen incompleta a recuperar, con dimensiones $N_1 \times N_2$, $N_1N_2 = N$.
	\item $\z$: versión en forma de vector(o señal) de la matriz $\Z$, con dimensiones $N \times 1$.
	\item $\P$: matriz de permutaci\'on de dimensiones $N \times N$.
\end{itemize}

Para obtener la una señal recuperada $\yhat$ a partir de $\z$ se procede de la siguiente forma:

\qquad

\begin{figure}[h]
	\centering
	\begin{equation*}
		\begin{array}[t]{ccccccccc}
		& & \mbox{\small{Permutaci\'on}} & & \mbox{\small{Operador de suavidad}} & & \mbox{\small{Permutaci\'on inversa}} & &\\
		\z & \longrightarrow & \boxed{\P} & \longrightarrow & \boxed{\H} & \longrightarrow & \boxed{\P^{-1}} & \longrightarrow & \yhat \\
		\end{array}
	\end{equation*}
	\caption{Esquema inicial}
	\label{fig:init_scheme}
\end{figure}

\qquad

Se reordenan los elementos de $\z$ seg\'un $\P$, y a la señal resultante $\z^p$ se le aplica un operador de suavidad $\H$ con el cual se obtienen los elementos faltantes, los elementos de dicho resultado se permutan nuevamente a su posici\'on inicial mediante $\P^{-1}$. Dicho vector final ser\'ia $\yhat$. Lo anterior puede expresarse como:

\begin{equation}
\yhat = \P^{-1}\H(\z^p) = \P^{-1}\H(\P\z)
\label{eq:yhat}
\end{equation}

Para comprender mejor lo que se quiere lograr con este procedimiento supongamos que contamos con la imagen real, la versi\'on de $\Z$ con la informaci\'on todos sus p\'ixeles. Aunque en la pr\'actica esta imagen no existe, la utilizaremos para reflejar de forma clara el funcionamiento del esquema inicial (figura \ref{fig:init_scheme}).

\begin{itemize}
	\item $\Y$: matriz de la imagen original, es igual a $\Z$ solo que si tiene el valor de sus p\'ixeles faltantes. 
	\item $\y$: versión en forma de vector(o señal) de la matriz $\Y$.
\end{itemize}

Supongamos que la matriz de permutaci\'on $\P$ tiene la propiedad de que al ser aplicada a $\y$ se obtiene una señal suave $\y^p$. Entonces, dado que $\z^p$ y $\y^p$ solo difieren en los p\'ixeles faltantes de $\Z$, y que $\H(z^p)$ completa la señal, haciéndola suave; se puede esperar que $\yhat$ sea una aproximaci\'on de $\y$. Dicho formalmente:

\begin{equation}
	\def\arraystretch{1.5}
	\begin{array}{lrcl}
	                                           &     \H (\P\z ) &\approx& \P\y        \\ 
	\Longrightarrow                            & P^{-1}\H(\P\z) &\approx& \P^{-1}\P\y \\
	\overset{(\ref{eq:yhat})}{\Longrightarrow} &          \yhat &\approx& \y          \\
	\end{array}
	\label{eq:permutation_smoothness}
\end{equation}

Llegados a este punto las únicas incógnitas son qu\'e operador $\H$ usar, y c\'omo obtener la matriz $\P$. Operadores para suavizar en \textbf{1D} hay varios en la literatura, ya sea los que se usan en interpolaci\'on o filtrado; por el momento dejemos este operador como un superpar\'ametro del algoritmo. Se explicar\'a entonces c\'omo hacer para obtener la permutaci\'on $\P$.

\section{La matriz de permutaci\'on}

El resultado (\ref{eq:permutation_smoothness}) tiene como condici\'on que $\y^p$ sea una señal suave. Para calcular la suavidad de $\y^p$ según (referencia a la formula de suavidad) ser\'ia:

\begin{equation}
\|\y^p\|_{\mathit{TV}} = \sum_{j = 2}^{N}|\y_j^{p} - \y_{j - 1}^{p}|
\label{eq:signal_smoothness}
\end{equation}

La matriz $\P$ que buscamos es tal que minimiza $\|\y^p\|_{\mathit{TV}}$, recordemos que no se cuenta con la señal  $\y$, entonces calcular $|\y_j^{p} - \y_{j - 1}^{p}|$ solo es posible cuando ambos elementos est\'an en la imagen incompleta $\Z$, lo que significa que conocemos su valor. Para encontrar la distancia entre dos p\'ixeles de $\Z$ (arbitrariamente de si se conoce su valor o no) usaremos sus parches de la siguiente manera:

\begin{equation}
|\y_j^{p} - \y_{j - 1}^{p}| \equiv \omega(\x_j^p,\; \x_{j - 1}^p)
\label{eq:omega_mean}
\end{equation}

Donde $\x_j^p$ denota el parche de $\Z$ cuyo p\'ixel central es denotado por $\z_j^p$. Y $\omega$ es una funci\'on de distancia definida sobre los parches la cual cumple que para cualesquiera dos parches, proximidad entre ellos sugiere proximidad entre sus p\'ixeles centrales, justo como se expresa en (\ref{eq:omega_mean}). Luego, el problema de minimizar $\|\y^p\|_{\mathit{TV}}$ por (\ref{eq:signal_smoothness}) y (\ref{eq:omega_mean}) es equivalente a minimizar:

\begin{equation}
\|\X^p\|_{\mathit{TV}} = \sum_{j = 2}^{N}\omega(\x_j^{p},\; \x_{j - 1}^{p})
\label{eq:path_smoothness}
\end{equation}

Donde $\X$ denota el vector de los parches de $\Z$ y $\X^p$ una permutaci\'on de $\X$ dada por una matriz $\P$. En conclusi\'on: la matriz $\P$ que buscamos es aquella  que minimiza $\|\X^p\|_{\mathit{TV}}$. Ahora bien, lo anterior se puede replantear de la siguiente forma. Si consideramos el grafo ponderado cuyos nodos son todos los parches de $\Z$ y la arista que une cada par de nodos $\x_i,\;\x_j$ tiene peso $\omega(\x_i,\; \x_j)$; debemos resolver una instancia del problema \textbf{NP}-completo del viajero, conocido como \TSP (del ingl\'es \textit{traveling salesman problem}) buscando un camino que comience en un parche cualquiera, pase por el resto de los parches una \'unica vez y cuya longitud es m\'inima.

\section{Algoritmo de reordenamiento de parches}

No se conoce ningún algoritmo con complejidad temporal polinomial para resolver \TSP, con lo cual en favor de lograr la eficiencia del esquema de la recuperaci\'on se tom\'o un algoritmo que encuentra una soluci\'on aproximada. El camino de parches que encuentra puede no ser el de m\'inima longitud, pero se garantiza que ser\'a de los menores. En cambio esta soluci\'on aproximada si se encuentra en un tiempo polinomial, que en este caso ser\'a lineal con respecto a la cantidad de parches o sea $O(\dim(\X))$.

Primeramente, se selecciona aleatoriamente un parche $\x_{j_0}$ por el cual comienza el camino. Luego se va iterando poniendo en cada paso un parche hasta completar el camino. En la iteraci\'on $k$ se explora la vecindad de tamaño $\B \times \B$ en la matriz $\Z$ alrededor del parche $\x_{j_{k - 1}}$ (que es el \'ultimo que se ha puesto). Ahora bien, existen dos casos:

\begin{itemize}
\item Todos los parches en esta vecindad ya están en el camino: se busca fuera de la vecindad los dos parches $\x_a,\; \x_b$ que no pertenecen al camino y cuyas distancias $\omega$ a $\x_{j_{k - 1}}$ son las dos menores.

\item Si existen parches disponibles en esa vecindad: se buscan de forma an\'aloga los parches $\x_a,\; \x_b$, esta vez dentro de la vecindad
\end{itemize}

Claramente si no es posible encontrar dos menores porque solo hay un parche disponible, entonces $\x_{j_k}$ ser\'ia ese \'unico parche. En cambio si se tienen $\x_a$ y $\x_b$ entonces:

\begin{equation}
\x_{j_k} = \left\{
	\begin{array}{ccc}
	\x_a &\mbox{con probabilidad}& p_a = \alpha e^{-\frac{\omega(\x_{j_{k - 1}},\; \x_a)}{\epsilon}}\\
	\x_b &\mbox{con probabilidad}& p_b = \alpha e^{-\frac{\omega(\x_{j_{k - 1}},\; \x_b)}{\epsilon}}\\
	\end{array}
\right.
\end{equation}

Donde $\alpha$ es tal que $p_a + p_b = 1$ y $\epsilon$ es un valor ajustable. Finalmente el conjunto de \'indices $\{j_i\}$ del camino de parches define la permutaci\'on $\P$ que buscamos. El pseudoc\'odigo se muestra en el algoritmo \ref{al:PRA}.

\begin{algorithm}
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead
	\KwIn{Todos los parches de la imagen: $\{\mathbf{x}_i\}_{i = 1}^M$}
	\KwOut{$\Omega$, reordenamiento del conjunto $ \{1,\;2,\;...,\;M\}$}
	$\Omega(1) \gets$ Un entero aleatorio en el intervalo $[1,\; M]$\;
	\For{$k \gets 1$ \textbf{to} $M - 1$} {
		$A_k \gets $ Conjuto de los \'indices de los $B \times B$ parches alrededor de $\x_{\Omega(k)}$\;
		\If{$|A_k \setminus \Omega| = 1$} {
			$\Omega(k + 1) \gets A_i \setminus \Omega$\;
		}
		\Else{
			\If{$|A_k \setminus \Omega| \ge 2$}{
				Encontrar $\x_a, \x_b$ los parches m\'as ceranos a $\x_{\Omega(k)}$ tales que $a, b \in |A_k \setminus \Omega|$\;
			}
			\Else{
				Encontrar $\x_a, \x_b$ los parches m\'as ceranos a $\x_{\Omega(k)}$ tales que $a, b \notin \Omega$\;
			}
			$\Omega(k + 1) \gets \left\{
				\begin{array}{ccc}
				\x_a &\mbox{con probabilidad}& p_a = \alpha e^{-\frac{\omega(\x_{j_{k - 1}},\; \x_a)}{\epsilon}}\\
				\x_b &\mbox{con probabilidad}& p_b = \alpha e^{-\frac{\omega(\x_{j_{k - 1}},\; \x_b)}{\epsilon}}\\
				\end{array}
			\right.$\;
		}
	}
	\Return{$\Omega$}\;
	\SetAlgoRefName{1}
	\caption{Reordenamiento de los parches}
	\label{al:PRA}
\end{algorithm}

\section{Trabajo con subim\'agenes}
Tomando como $n$ el tamaño de los parches en este procedimiento, tenemos por (referencia a cap 1) que la cantidad de parches de la matriz $\Z$ es: 

\begin{equation}
N_p = (N_1 - \sqrt{n} + 1)(N_2 - \sqrt{n} + 1)
\label{eq:patches}
\end{equation}

Es claro que $n > 1$, pues los parches cubren pequeñas zonas de p\'ixeles, con lo cual se cumple la siguiente desigualdad:

\begin{equation}
	\begin{array}{lrcl}
	                &                 1 &<& n        \\ 
	\Longrightarrow &                 1 &<& \sqrt{n} \\
	\Longrightarrow &     -\sqrt{n} + 1 &<& 0        \\
	\Longrightarrow & N_1 -\sqrt{n} + 1 &<& N_1      \\
	\end{array}
\end{equation}

An\'alogamente $N_2 -\sqrt{n} + 1 < N_2$, multiplicando estas dos desigualdades obtenemos que:

\begin{equation}
	\begin{array}{lrcl}
	                & (N_1 - \sqrt{n} + 1)(N_2 - \sqrt{n} + 1) &<& N_1N_2 \\ 
	\Longrightarrow &                                      N_p &<& N      \\
	\label{eq:patches_ineq}
	\end{array}
\end{equation}

La cantidad de parches $N_p$ de $\Z$ es menor que el tamaño del vector $\z$, luego las dimensiones de la matriz $\P$ obtenidas con algoritmo \ref{al:PRA} son de $N_p \times N_p$; lo que implica que el esquema inicial (figura \ref{fig:init_scheme}) solo ser\'ia aplicables a señales de tamaño $N_p \times 1$.

\begin{figure}[h]
	\[\Z = \left(
	\begin{matrix}
	a & b & c & d\\\cline{2-3}
	e & \multicolumn{1}{|c}{f} & \multicolumn{1}{c|}{g} & h\\
	i & \multicolumn{1}{|c}{j} & \multicolumn{1}{c|}{k} & l\\\cline{2-3}
	m & n & o & p\\
	\end{matrix}
	\right)
	\qquad\X = \left(
	\begin{matrix}\cline{5-5}
	a & b & c & e & \multicolumn{1}{|c|}{f} & g & i & j & k\\
	b & c & d & f & \multicolumn{1}{|c|}{g} & h & j & k & l\\
	e & f & g & i & \multicolumn{1}{|c|}{j} & k & m & n & o\\
	f & g & h & g & \multicolumn{1}{|c|}{k} & l & n & o & p\\\cline{5-5}
	\end{matrix}
	\right)
	\]
\end{figure}

Tengamos en cuenta que la subimagen formada por los p\'ixeles centrales de cada parche es una señal de tamaño $N_p$, entonces podr\'iamos aplicarle el esquema (figura \ref{fig:init_scheme}) para recuperar por completo esa subimagen. Considerando todos los posibles centros para los parches, tendremos un total de $n$ subim\'agenes (referencia al capitulo 1). Si recuperamos cada una de estas usando el esquema inicial, podemos usar esas subim\'agenes recuperadas para colocarlas en su posición natural en $\Z$, est\'a claro que todas se solapan, pero para ello promediamos los p\'ixeles solapados, y así se formar\'ia una imagen completa recuperada.  

Si vemos al vector de parches $X$ como una matriz, poniendo los parches como columnas, entonces cada fila de esta matriz es una submimagen en forma de vector. Tal y como muestra es siguiente ejemplo

\begin{equation}
	\z\;\left\{
	\def\arraystretch{2.2}
	\begin{array}{ccccccccc}
		\longrightarrow & \boxed{\P_1} & \longrightarrow & \boxed{\H} & \longrightarrow & \boxed{\P_1^{-1}} & \longrightarrow \\
		\longrightarrow & \boxed{\P_2} & \longrightarrow & \boxed{\H} & \longrightarrow & \boxed{\P_2^{-1}} & \longrightarrow \\
		\longrightarrow & \huge\vdots &  & \huge\vdots &  & \huge\vdots & \longrightarrow & \\
		\longrightarrow & \boxed{\P_K} & \longrightarrow & \boxed{\H} & \longrightarrow & \boxed{\P_K^{-1}} & \longrightarrow
	\end{array}
	\right\}\;\oplus\longrightarrow\boxed{\times\frac{1}{K}}\longrightarrow\yhat
\end{equation}

%	Ejemplo de algoritmo:
%	
%	\begin{algorithm}
%	    \caption{Algoritmo de Arnoldi para construir una base ortonormal del subespacio de Krylov $\mathcal{K}_\mf(\tau A,b)$}
%	    \label{alg:Arnoldi}
%	    \KwIn{Matriz $A\in \mathbb{R}^{d\times d}$, vector $b\in \mathbb{R}^{d}$ y constante $\tau$}
%	    \KwOut{Base ortonormal $\{v_1,v_2,\ldots,v_\mf,v_{\mf+1}\}$ de $K_\mf(\tau A,b)$ y matriz de Hessenberg $H_\mf=V_\mf^{\intercal}\tau A V_\mf $,
%	        donde $V_{\mf+1}=[v_1\,v_2\,\cdots \,v_\mf\,v_{\mf+1}]\in \mathbb{R}^{d\times \mf+1}$, $breakdown$ }
%	    $breakdown=false$\\
%	    $v_1=b/\lVert b \rVert_2$\\
%	    \For{ $j=1,2,\ldots,\mf$ }{
%	        $w_j=\tau A v_j$\\
%	        \For{ $i=1,\ldots,j$}{ 
%	            $\hf_{ij}=\langle w_j,v_i \rangle$\\
%	            $w_j=w_j-\hf_{ij}v_i$       
%	        }
%	        $\hf_{j+1,i}=\lVert w_j \rVert_2$\\
%	        \eIf(\tcp*[h] \emph{Break Down}\label{alg:breakdown}){$\hf_{j+1,i}<2\epsilon_{mach}$}{
%	            $\mf_{cut}=j$\\
%	             $breakdown=true$\\
%	            Stop
%	        }{
%	            $v_{j+1}=w_j/\hf_{j+1,i}$
%	        }
%	    }
%	\end{algorithm}


